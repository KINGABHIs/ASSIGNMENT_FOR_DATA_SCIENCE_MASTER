{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec961115-e219-4016-b35a-fb508a47aed2",
   "metadata": {},
   "source": [
    "##  Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "### Ans :\n",
    "Lasso Regression is differ form other regression tecniques because, it is useful Feature selection, Regularization, and handling the overfitting problem.\n",
    "\n",
    "1. Feature Selection:\n",
    "Lasso can eliminate irrelevant features by setting their coefficients to zero, while techniques like Ridge Regression only shrink coefficients but do not eliminate them.\n",
    "\n",
    "2. Regularization:\n",
    "Lasso uses the L1 penalty (absolute values of coefficients), while Ridge Regression uses the L2 penalty (squares of coefficients).\n",
    "\n",
    "3. Overfitting Control:\n",
    "Both Lasso and Ridge help control overfitting by penalizing large coefficients, but Lasso is more effective in feature selection when you have many features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94163868-5281-4f14-bb49-19d68d2ec53f",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "### Ans :\n",
    "The main advantage of Lasso Regression in feature selection is that it can set some coefficients to zero, effectively eliminating irrelevant or less important features. This helps in creating a simpler model by selecting only the most important features, which can improve model performance and reduce overfitting.\n",
    "\n",
    "It is useful because:\n",
    "1. Sparsity: Lasso produces sparse models, meaning it automatically chooses a smaller subset of features.\n",
    "2. Improved interpretability: With fewer features, the model becomes easier to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac358a3-2db4-4a08-8c64-cb268edf0e6d",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "### Ans :\n",
    "Interpreting the Coefficients of a Lasso Regression Model:\n",
    "* Magnitude of Coefficients: The magnitude of the coefficients indicates how much each feature influences the target variable. Larger coefficients have a stronger impact, while smaller coefficients have less influence.\n",
    "\n",
    "* Zero Coefficients: Coefficients that are set to zero by Lasso regression indicate that those features are not important for the model and have been excluded from the prediction.\n",
    "\n",
    "* Feature Selection: Lasso regression automatically selects a subset of features. Features with non-zero coefficients are considered important for predicting the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ca70c-2b6e-45f7-823f-24dcf6791b8c",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "### Ans :\n",
    "Lambda (λ): \n",
    "\n",
    "Description: Lambda is the regularization parameter that controls the strength of the penalty applied to the coefficients.\n",
    "\n",
    "Effect on Model: \n",
    "* Higher λ: Increases regularization, shrinking more coefficients to zero. This leads to a simpler model with fewer features but can cause underfitting if too high.\n",
    "\n",
    "* Lower λ: Reduces regularization, allowing the coefficients to remain larger. This can lead to a more complex model, potentially overfitting the data if too low.\n",
    "\n",
    "Max Iterations:\n",
    "\n",
    "* Description: Specifies the maximum number of iterations allowed for the optimization process.\n",
    "* Effect on Model: A higher value ensures that the algorithm converges properly, but it may increase computation time.\n",
    "\n",
    "Affect Performance:\n",
    "λ (Lambda) helps balance bias and variance. A well-tuned λ improves the model's ability to generalize, while improper tuning can lead to underfitting (too large) or overfitting (too small).\n",
    "Max iterations ensures the model converges to a solution, especially with larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40279af-837d-46bc-91e7-24293a3a0829",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "### Ans :\n",
    "Yes, Lasso Regression can be used for non-linear regression problems, but with a modification.\n",
    "\n",
    "working :\n",
    "* Lasso is inherently a linear model, meaning it assumes a linear relationship between features and the target.\n",
    "* To handle non-linear relationships, you can transform the features (e.g., using polynomial features, logarithms, or interactions between variables) before applying Lasso. This allows the model to capture non-linear patterns.\n",
    "\n",
    "Example:\n",
    "If you have a feature X, you can create additional features like X^2, 𝑋^3 , or interaction terms like \n",
    "𝑋 * 𝑌. Lasso can then shrink and select these transformed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335b243-ede6-4c8d-9d6e-0eea0a9cd0e0",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "### Ans :\n",
    "Difference Between Ridge Regression and Lasso Regression:\n",
    "1. Regularization Type:\n",
    "\n",
    "* Ridge Regression: Uses L2 regularization (squared penalty on coefficients), which shrinks coefficients but never sets them to zero.\n",
    "* Lasso Regression: Uses L1 regularization (absolute value penalty on coefficients), which can shrink coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "2. Effect on Coefficients:\n",
    "\n",
    "* Ridge: Shrinks all coefficients, but none become exactly zero.\n",
    "* Lasso: Can eliminate (set to zero) unimportant features, leading to a simpler model.\n",
    "\n",
    "3. Feature Selection:\n",
    "\n",
    "* Ridge: Does not perform feature selection; keeps all features in the model.\n",
    "* Lasso: Automatically selects important features by setting less important ones to zero.\n",
    "\n",
    "4. Use Case:\n",
    "\n",
    "* Ridge: Better for situations where all features are useful but may be highly correlated.\n",
    "* Lasso: Better when you have many features and want to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab295ff0-af12-4f95-8df5-7aced5ff45c1",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "### Ans :\n",
    "Yes, Lasso Regression can help handle multicollinearity in the input features.\n",
    "\n",
    "working :\n",
    "* Multicollinearity occurs when input features are highly correlated with each other, making it difficult for models to estimate their coefficients accurately.\n",
    "* Lasso regression addresses this by shrinking the coefficients of correlated features and can set some coefficients to zero, effectively removing redundant features.\n",
    "* This feature selection process helps reduce the impact of correlated features, making the model more stable and improving its ability to generalize.\n",
    "\n",
    "Lasso can handle multicollinearity by eliminating or reducing the effect of highly correlated features through feature selection, resulting in a simpler and more stable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c91b8b-de16-4a81-bdce-fb0252c57c26",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "### Ans :\n",
    "To choose the optimal value of lambda (λ) in Lasso Regression, you can use cross-validation.\n",
    "\n",
    "Steps:\n",
    "1. Split the data into training and validation sets (or use k-fold cross-validation).\n",
    "2. Test different values of λ (e.g., 0.1, 1, 10, etc.) by training the model with each value.\n",
    "3. Evaluate model performance using a metric like Mean Squared Error (MSE) or R-squared on the validation set.\n",
    "4. Choose the λ that gives the best performance (lowest error) on the validation set.\n",
    "\n",
    "Cross-validation helps to find the λ that balances bias and variance, giving the model the best ability to generalize to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
