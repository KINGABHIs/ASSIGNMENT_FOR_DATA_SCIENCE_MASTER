{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f319248-e7ea-42b3-ad6b-25a616a0a97d",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "### Ans: \n",
    "Min-Max scaling, also called normalization, is a data preprocessing technique that rescales features to a fixed range, typically[0,1].It is a linear transformation method used to bring all feature values into a specific range, ensuring no feature dominates others due to differing scales.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "> 𝑋scaled = 𝑋−𝑋min /𝑋max −𝑋min\n",
    "\n",
    "        Where: X is the original feature value.\n",
    "\n",
    "        𝑋min and 𝑋max are the minimum and maximum values of the feature.\n",
    "\n",
    "Used TO : \n",
    "1. Prevents Dominance: Features with large magnitudes do not dominate features with smaller magnitudes.\n",
    "2. Speeds Up Convergence: Algorithms like gradient descent converge faster when data is normalized.\n",
    "3. Ensures Compatibility: Many machine learning algorithms (e.g., KNN, SVM) perform better with normalized data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset with the following feature values for \"Age\":\n",
    "\n",
    "Ages= [18,22,35,60,45]\n",
    "\n",
    "Steps for Min-Max scaling to the range [0, 1]:\n",
    "\n",
    "Find 𝑋min=18X min​ =18 and 𝑋max=60\n",
    "\n",
    "Apply the formula:\n",
    "\n",
    "𝑋scaled =𝑋−18 / 60−18​\n",
    "\n",
    "​Result:Scaled Ages=[0,0.095,0.405,1,0.643]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65425ad-3f8e-46a3-ab68-350bde63932e",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?Provide an example to illustrate its application.\n",
    "\n",
    "### Ans :\n",
    "\n",
    "Unit Vector Technique in Feature Scaling\n",
    "The Unit Vector technique, also known as Normalization, scales feature vectors so that their length (Euclidean norm) is equal to 1. This ensures that the scaled data maintains its direction while standardizing the magnitude.\n",
    "\n",
    "The formula for the unit vector scaling of a feature vector X is:\n",
    ">  𝑋scaled=𝑋 / ∥𝑋∥\n",
    "\n",
    "   * Where:𝑋 is the original feature vector.\n",
    "   * ∥𝑋∥ is the Euclidean norm of X, calculated as ∥𝑋∥\n",
    "\n",
    "Difference between min-max scaling and unit vector\n",
    "1. Min-Max Scaling\n",
    "* \tScales data to a fixed range (e.g., [0, 1]).\n",
    "* \tMay alter direction (angle) of the data.\n",
    "* \tWhen feature magnitudes differ widely.\n",
    "   \n",
    "2. Unit Vector Scaling\n",
    "* Scales the vector to have a unit norm (length of 1).\n",
    "* Maintains direction of the vector.\n",
    "* When angles or relative directions are more important.\n",
    "\n",
    "\n",
    "Example\n",
    "Dataset:\n",
    "Suppose a 2D feature vector 𝑋=[3,4].\n",
    "\n",
    "Unit Vector Scaling:\n",
    "1. Compute the Euclidean norm:\n",
    ">    ∥𝑋∥=√3^2 + 4^2 = 5\n",
    "\n",
    "2. Scale each component:\n",
    "\n",
    ">    𝑋scaled=𝑋 /∥𝑋∥=[3/5, 4/5]=[0.6,0.8]\n",
    "\n",
    "Result:\n",
    "The scaled vector [0.6,0.8] has the same direction as the original [3,4] but a magnitude of 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55bd2a-20ef-4d06-9d0e-844a9cfbabad",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "### Ans :\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and data analysis. It transforms a dataset with correlated features into a smaller set of uncorrelated features called principal components while retaining as much of the dataset's variability (information) as possible.\n",
    "\n",
    "### How PCA is Used in Dimensionality Reduction\n",
    "PCA reduces the number of features while preserving most of the variability:\n",
    "\n",
    "High-dimensional datasets (e.g., with hundreds of features) are transformed into a few principal components.\n",
    "By reducing dimensions, PCA simplifies models, speeds up computations, and avoids overfitting.\n",
    "\n",
    "### Example\n",
    "Dataset:\n",
    "Consider a dataset with two highly correlated features:\n",
    "𝑋 = [2 3 4 5 , 3 5 7 9]\n",
    "\n",
    "Applying PCA:\n",
    "1. Standardize Data: Center the mean.\n",
    "2. Compute Covariance Matrix:\n",
    "Cov(𝑋)=[1.67  3.33 , 3.33  6.67]\n",
    "3. Find Eigenvalues and Eigenvectors:\n",
    "Eigenvalues:8, 0.34\n",
    "Eigenvectors: [0.45,0.89], [−0.89,0.45]\n",
    "Select Principal Components: Choose the component with the largest eigenvalue (explains 95% variance).\n",
    "Transform Data: Project original data onto this component.\n",
    "Result:\n",
    "The data is reduced from 2D to 1D while retaining most information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5402e-c22a-40f5-8703-f4d760ccb700",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "### Ans : \n",
    "\n",
    "Relationship Between PCA and Feature Extraction\n",
    "PCA is a feature extraction technique.:It transforms the original features into new features (called principal components) that are linear combinations of the original ones.These components retain the most important information (variance) from the data while reducing redundancy.\n",
    "\n",
    "How PCA is Used for Feature Extraction :PCA identifies the directions (principal components) with the most variance in the data. It uses these components as new features that summarize the dataset effectively.Instead of using the original features, the top principal components are used in the analysis or model.\n",
    "\n",
    "\n",
    "Example\n",
    "Dataset:\n",
    "A dataset with 3 features: Height, Weight, and BMI (Body Mass Index). These features are correlated.\n",
    "\n",
    "Using PCA:\n",
    "PCA finds that most of the variance can be captured by 2 components:\n",
    "PC1: Represents overall size (Height + Weight).\n",
    "PC2: Represents BMI-related variance.\n",
    "The 3 features are reduced to 2 components (PC1, PC2), keeping most of the information.\n",
    "Result:\n",
    "You now work with PC1 and PC2 instead of the original 3 features, simplifying the dataset while preserving key information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff1e0a-0308-4e1f-b4c2-01563c8b729a",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "### Ans :\n",
    "Min-Max scaling ensures that features like price, rating, and delivery time are brought to the same range (e.g., [0, 1]) so no single feature dominates the others due to differing scales.\n",
    "\n",
    "Steps to Apply Min-Max Scaling\n",
    "\n",
    "1. Identify Features: Use features like:\n",
    "\n",
    "Price (₹): Range may be 100 to 1000.\n",
    "Rating: Range may be 1 to 5.\n",
    "Delivery Time (mins): Range may be 10 to 60.\n",
    "\n",
    "2.Apply the Formula:\n",
    "\n",
    ">    𝑋scaled=𝑋−𝑋min /𝑋max−𝑋min\n",
    "\n",
    "3. Scale Each Feature:\n",
    "\n",
    "* For Price: Scale values between 0 and 1 using its min and max.\n",
    "* For Rating: Normalize the 1–5 scale.\n",
    "* For Delivery Time: Bring it to a common scale.\n",
    "\n",
    "4. Use the Scaled Data: The recommendation system now processes all features on the same scale, improving performance and fairness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc00af5-e479-45cc-b462-ead2019ed7eb",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "### Ans :\n",
    "PCA for Stock Price Prediction : PCA helps reduce the number of features (dimensionality) while keeping the most important information, making the model simpler and faster.\n",
    "\n",
    "### Steps to Use PCA:\n",
    "1. Standardize the Data:\n",
    "\n",
    "Make all features (e.g., revenue, profits, market trends) have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. Compute Principal Components:\n",
    "\n",
    "PCA identifies new features (principal components) that capture the most variance in the data.\n",
    "\n",
    "3. Select Top Components:\n",
    "\n",
    "Choose a few components that explain most of the variance (e.g., 95% of the total variance).\n",
    "\n",
    "4. Transform the Data: Replace the original features with the selected principal components.\n",
    "\n",
    "Example:\n",
    "* Original dataset: 50 features (e.g., revenue, expenses, market index changes).\n",
    "* PCA reduces it to 10 components, keeping most of the important information.\n",
    "\n",
    "Result:\n",
    "* The reduced dataset is smaller and faster to process.\n",
    "* The model focuses on key patterns, improving performance and reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a774235-ecff-45f8-a5f7-dfbc23503f06",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "### Ans :\n",
    "\n",
    "Steps to Perform Min-Max Scaling (Range: -1 to 1)\n",
    "The formula for Min-Max scaling to a range [a,b] is:\n",
    "\n",
    ">    𝑋scaled=𝑎+(𝑋−𝑋min) /(𝑋max−𝑋min)*(𝑏−𝑎)\n",
    "\n",
    "        Here:𝑎=−1, 𝑏=1, 𝑋min=20\n",
    "\n",
    "Transform Each Value:\n",
    "\n",
    "For each value X:\n",
    "\n",
    ">     𝑋scaled=−1+(𝑋−1) /(20−1)*(1−(−1))\n",
    "\n",
    "For 𝑋=1:\n",
    ">     𝑋scaled=−1+(1−1) /(20−1)*2=−1 \n",
    "\n",
    "For X=5:\n",
    ">     𝑋scaled=−1+(5−1) /(20−1)*2=−0.79\n",
    "\n",
    "For X=10:\n",
    ">     𝑋scaled=−1+(10−1) /(20−1)*2=−0.42\n",
    "\n",
    "For X=15:\n",
    ">     𝑋scaled=−1+(15−1) /(20−1)*2=0.16\n",
    "\n",
    "For X=20:\n",
    ">     𝑋scaled=−1+(20−1) /(20−1)*2=1\n",
    "\n",
    "Final Scaled Values\n",
    "[−1,−0.79,−0.42,0.16,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654cb8c-2b4c-4b5f-98ff-b46b33ff63f9",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "### Ans :\n",
    "Steps to Perform PCA for Feature Extraction:\n",
    "\n",
    "1. Standardize the Data: Ensure all features (height, weight, age, etc.) are scaled properly.\n",
    "\n",
    "2. Compute Principal Components: PCA will calculate components that capture the variance in the dataset.\n",
    "\n",
    "3. Choose the Number of Principal Components:\n",
    "\n",
    "* Use the explained variance ratio, which shows how much variance each component captures.\n",
    "* Retain components that explain 95% or more of the total variance.\n",
    "\n",
    "\n",
    "How Many Components to Retain?\n",
    "If the explained variance ratio looks like this:\n",
    "PC1: 50%\n",
    "PC2: 30%\n",
    "PC3: 15%\n",
    "PC4: 4%\n",
    "PC5: 1%\n",
    "Retain the top 3 components (50% + 30% + 15% = 95%).\n",
    "\n",
    "why ?\n",
    "The top 3 components capture most of the important information, reducing dimensions while keeping meaningful data. This simplifies the model without losing key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6a6a2-6f98-43ca-9be8-67f51b93f2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
